{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import pygame\n",
    "import random\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MAX_VEL_X = 5.0\n",
    "MAX_VEL_Y = 5.0\n",
    "\n",
    "class LunarLandingEnv(gym.Env):\n",
    "    metadata = {'render_modes': ['human'], 'render_fps': 60}\n",
    "\n",
    "    def __init__(self):\n",
    "        super(LunarLandingEnv, self).__init__()\n",
    "\n",
    "        # Define action and observation space\n",
    "        # Actions: 0 - Do nothing, 1 - Thrust Up, 2 - Thrust Left, 3 - Thrust Right\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "\n",
    "        # Observations: x, y, velocity_x, velocity_y, fuel, landing_center_norm\n",
    "        low = np.array([-1, -1, -1, -1, 0, -1], dtype=np.float32)\n",
    "        high = np.array([1, 1, 1, 1, 1, 1], dtype=np.float32)\n",
    "        self.observation_space = spaces.Box(low, high, dtype=np.float32)\n",
    "\n",
    "        # Game parameters\n",
    "        self.gravity = 0.05\n",
    "        self.thrust_vertical = 0.1\n",
    "        self.thrust_horizontal = 0.05\n",
    "        self.max_fuel = 1000  # Increased fuel\n",
    "\n",
    "        # Pygame initialization\n",
    "        self.screen_width = 800\n",
    "        self.screen_height = 600\n",
    "        self.screen = None\n",
    "        self.clock = None\n",
    "        self.is_open = True\n",
    "\n",
    "        # Game outcome\n",
    "        self.last_outcome = None  # Will be 'victory' or 'defeat' when the game ends\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.x = self.screen_width / 2  # Start at the center\n",
    "        self.y = 50\n",
    "        self.vel_x = 0.0\n",
    "        self.vel_y = 0.0\n",
    "        self.fuel = self.max_fuel\n",
    "        self.landing_zone = self._choose_landing_zone()\n",
    "        self.current_step = 0  # Step counter\n",
    "        self.max_steps = 1000  # Max steps per episode\n",
    "        self.last_outcome = None  # Reset outcome\n",
    "        return self._get_obs(), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        # Apply action\n",
    "        if action == 1 and self.fuel > 0:  # Thrust Up\n",
    "            self.vel_y -= self.thrust_vertical\n",
    "            self.fuel -= 1\n",
    "        elif action == 2 and self.fuel > 0:  # Thrust Left\n",
    "            self.vel_x -= self.thrust_horizontal\n",
    "            self.fuel -= 1\n",
    "        elif action == 3 and self.fuel > 0:  # Thrust Right\n",
    "            self.vel_x += self.thrust_horizontal\n",
    "            self.fuel -= 1\n",
    "\n",
    "        # Update physics\n",
    "        self.vel_y += self.gravity\n",
    "        self.y += self.vel_y\n",
    "        self.x += self.vel_x\n",
    "\n",
    "        # Impose horizontal limits\n",
    "        self.x = max(20, min(self.x, 780))\n",
    "\n",
    "        # Increment step counter\n",
    "        self.current_step += 1\n",
    "\n",
    "        # Initialize reward and constants\n",
    "        reward = 0.0\n",
    "        k_y = 5  # Weight for vertical velocity adjustment\n",
    "        k_x = 4   # Weight for horizontal velocity adjustment\n",
    "        k_dir = 3  # Weight to penalize wrong direction\n",
    "        k_dist = 1  # Weight for distance from center\n",
    "\n",
    "        # Calculate landing zone center\n",
    "        landing_center = (self.landing_zone[0] + self.landing_zone[1]) / 2\n",
    "        distance_to_center_x = self.x - landing_center\n",
    "\n",
    "        # Reward for low and positive Y velocity\n",
    "        if 0.5 <= self.vel_y <= 1:\n",
    "            reward += k_y  # Max reward for ideal velocity\n",
    "        else:\n",
    "            reward -= k_y * abs(self.vel_y - 0.75)  # Penalty proportional to deviation from 0.75\n",
    "\n",
    "        if self.x < landing_center:  # Lander is to the left\n",
    "            if self.vel_x < 0.4:\n",
    "                if action == 3:  # Pushing to the right\n",
    "                    reward += k_x\n",
    "                elif action == 2:  # Pushing to the left\n",
    "                    reward -= k_dir\n",
    "\n",
    "        elif self.x > landing_center:  # Lander is to the right\n",
    "            if self.vel_x > -0.4:\n",
    "                if action == 2:  # Pushing to the left\n",
    "                    reward += k_x\n",
    "                elif action == 3:  # Pushing to the right\n",
    "                    reward -= k_dir\n",
    "\n",
    "        # Normalize reward between -10 and 10\n",
    "        reward = max(-10, min(10, reward))\n",
    "\n",
    "        # Check for termination\n",
    "        terminated = False\n",
    "        if self.y >= 530:  # Lander has reached the platform height\n",
    "            self.y = 530  # Adjust position to exactly on platform\n",
    "            self.vel_y = 0  # Stop vertical movement\n",
    "            self.vel_x = 0  # Stop horizontal movement\n",
    "            # Determine if landing is successful\n",
    "            landing_success_threshold = 20  # pixels\n",
    "            landing_velocity_threshold = 0.4  # units\n",
    "            if abs(self.x - landing_center) <= landing_success_threshold and abs(self.vel_y) <= landing_velocity_threshold:\n",
    "                # Successful landing\n",
    "                self.last_outcome = 'victory'\n",
    "            else:\n",
    "                # Unsuccessful landing\n",
    "                self.last_outcome = 'defeat'\n",
    "            terminated = True\n",
    "\n",
    "        elif self.current_step >= self.max_steps:\n",
    "            terminated = True\n",
    "            self.last_outcome = 'defeat'\n",
    "        elif self.y > self.screen_height or self.x <= 20 or self.x >= 780:\n",
    "            terminated = True\n",
    "            reward = -10  # Max penalty for going out of bounds\n",
    "            self.last_outcome = 'defeat'\n",
    "\n",
    "        # Return observations, reward, and episode info\n",
    "        return self._get_obs(), reward, terminated, False, {}\n",
    "\n",
    "    def render(self):\n",
    "        if self.screen is None:\n",
    "            pygame.init()\n",
    "            pygame.display.init()\n",
    "            self.screen = pygame.display.set_mode(\n",
    "                (self.screen_width, self.screen_height)\n",
    "            )\n",
    "        if self.clock is None:\n",
    "            self.clock = pygame.time.Clock()\n",
    "\n",
    "        if self.last_outcome is None:\n",
    "            # Normal game rendering\n",
    "            self.screen.fill((0, 0, 0))  # Clear screen\n",
    "\n",
    "            # Draw game elements\n",
    "            self._draw_stars()\n",
    "            self._draw_terrain()\n",
    "            self._draw_lander()\n",
    "\n",
    "            # Show game state information\n",
    "            landing_center = (self.landing_zone[0] + self.landing_zone[1]) / 2\n",
    "            distance_to_center = self.x - landing_center\n",
    "\n",
    "            # Display rocket coordinates and state\n",
    "            font = pygame.font.Font(None, 36)\n",
    "            info_text = [\n",
    "                f\"Rocket X: {self.x:.2f}\",\n",
    "                f\"Rocket Y: {self.y:.2f}\",\n",
    "                f\"Distance to Center: {distance_to_center:.2f}\",\n",
    "                f\"Velocity X: {self.vel_x:.2f}\",\n",
    "                f\"Velocity Y: {self.vel_y:.2f}\",\n",
    "                f\"Fuel: {self.fuel}\",\n",
    "                f\"Landing Center X: {landing_center:.2f}\",\n",
    "            ]\n",
    "            for i, text in enumerate(info_text):\n",
    "                surface = font.render(text, True, (255, 255, 255))\n",
    "                self.screen.blit(surface, (10, 10 + i * 30))\n",
    "\n",
    "            # Visual marker for landing platform center\n",
    "            pygame.draw.circle(\n",
    "                self.screen, (0, 255, 0), (int(landing_center), 550), 5\n",
    "            )\n",
    "\n",
    "            # Display rocket's current coordinates next to it\n",
    "            coord_text = f\"({int(self.x)}, {int(self.y)})\"\n",
    "            coord_surface = font.render(coord_text, True, (255, 255, 255))\n",
    "            self.screen.blit(coord_surface, (int(self.x) + 20, int(self.y) - 40))\n",
    "\n",
    "            # Update display\n",
    "            pygame.display.flip()\n",
    "            self.clock.tick(self.metadata[\"render_fps\"])\n",
    "        else:\n",
    "            # Draw victory or defeat screen\n",
    "            self._draw_end_screen()\n",
    "\n",
    "    def _draw_end_screen(self):\n",
    "        # Clear screen\n",
    "        self.screen.fill((0, 0, 0))\n",
    "\n",
    "        # Victory or defeat text\n",
    "        font = pygame.font.Font(None, 74)\n",
    "        if self.last_outcome == 'victory':\n",
    "            text = font.render(\"YOU WIN!\", True, (0, 255, 0))\n",
    "        else:\n",
    "            text = font.render(\"GAME OVER\", True, (255, 0, 0))\n",
    "        text_rect = text.get_rect(center=(self.screen_width // 2, self.screen_height // 2))\n",
    "        self.screen.blit(text, text_rect)\n",
    "\n",
    "        # Update display\n",
    "        pygame.display.flip()\n",
    "\n",
    "    def close(self):\n",
    "        if self.screen is not None:\n",
    "            pygame.display.quit()\n",
    "            pygame.quit()\n",
    "            self.is_open = False\n",
    "\n",
    "    # Helper methods\n",
    "    def _choose_landing_zone(self):\n",
    "        start = random.randint(100, 600)\n",
    "        width = random.randint(50, 150)\n",
    "        end = start + width\n",
    "        return (start, end)\n",
    "\n",
    "    def _get_obs(self):\n",
    "        x_norm = (self.x - self.screen_width / 2) / (self.screen_width / 2)\n",
    "        y_norm = (self.y - self.screen_height / 2) / (self.screen_height / 2)\n",
    "        vel_x_norm = self.vel_x / MAX_VEL_X\n",
    "        vel_y_norm = self.vel_y / MAX_VEL_Y\n",
    "        fuel_norm = self.fuel / self.max_fuel\n",
    "        landing_center_norm = (self.landing_zone[0] + self.landing_zone[1]) / 2 / self.screen_width\n",
    "        return np.array(\n",
    "            [x_norm, y_norm, vel_x_norm, vel_y_norm, fuel_norm, landing_center_norm],\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "\n",
    "    def _draw_stars(self):\n",
    "        # Draw static stars\n",
    "        if not hasattr(self, \"stars\"):\n",
    "            self.stars = [\n",
    "                (random.randint(0, 800), random.randint(0, 600)) for _ in range(50)\n",
    "            ]\n",
    "        for star in self.stars:\n",
    "            pygame.draw.circle(self.screen, (255, 255, 255), star, 2)\n",
    "\n",
    "    def _draw_terrain(self):\n",
    "        # Draw terrain and landing zone\n",
    "        pygame.draw.polygon(\n",
    "            self.screen,\n",
    "            (100, 100, 100),\n",
    "            [\n",
    "                (0, 550),\n",
    "                (200, 450),\n",
    "                (400, 500),\n",
    "                (600, 450),\n",
    "                (800, 550),\n",
    "                (800, 600),\n",
    "                (0, 600),\n",
    "            ],\n",
    "        )\n",
    "        # Draw landing zone\n",
    "        start, end = self.landing_zone\n",
    "        pygame.draw.rect(\n",
    "            self.screen, (150, 150, 150), (start, 550, end - start, 50)\n",
    "        )\n",
    "        # Flags\n",
    "        pygame.draw.line(self.screen, (255, 255, 255), (start, 550), (start, 520), 2)\n",
    "        pygame.draw.polygon(\n",
    "            self.screen,\n",
    "            (255, 255, 0),\n",
    "            [(start, 520), (start + 15, 527), (start, 534)],\n",
    "        )\n",
    "        pygame.draw.line(self.screen, (255, 255, 255), (end, 550), (end, 520), 2)\n",
    "        pygame.draw.polygon(\n",
    "            self.screen,\n",
    "            (255, 255, 0),\n",
    "            [(end, 520), (end - 15, 527), (end, 534)],\n",
    "        )\n",
    "\n",
    "    def _draw_lander(self):\n",
    "        x = int(self.x)\n",
    "        y = int(self.y)\n",
    "\n",
    "        # Draw the lander\n",
    "        pygame.draw.rect(\n",
    "            self.screen, (200, 200, 200), (x - 10, y - 30, 20, 60)\n",
    "        )  # Main body\n",
    "        pygame.draw.polygon(\n",
    "            self.screen,\n",
    "            (150, 150, 150),\n",
    "            [(x - 10, y + 20), (x - 20, y + 30), (x - 10, y + 30)],\n",
    "        )  # Left wing\n",
    "        pygame.draw.polygon(\n",
    "            self.screen,\n",
    "            (150, 150, 150),\n",
    "            [(x + 10, y + 20), (x + 20, y + 30), (x + 10, y + 30)],\n",
    "        )  # Right wing\n",
    "        pygame.draw.polygon(\n",
    "            self.screen,\n",
    "            (255, 0, 0),\n",
    "            [(x, y - 40), (x - 10, y - 30), (x + 10, y - 30)],\n",
    "        )  # Top cone\n",
    "\n",
    "        # Draw fuel level indicator\n",
    "        fuel_percentage = self.fuel / self.max_fuel\n",
    "        fuel_bar_height = 50\n",
    "        fuel_bar_width = 10\n",
    "        pygame.draw.rect(\n",
    "            self.screen,\n",
    "            (255, 255, 255),\n",
    "            (x - 25, y - 30, fuel_bar_width, fuel_bar_height),\n",
    "            1,\n",
    "        )  # Fuel bar border\n",
    "        pygame.draw.rect(\n",
    "            self.screen,\n",
    "            (0, 255, 0),\n",
    "            (\n",
    "                x - 25,\n",
    "                y - 30 + fuel_bar_height * (1 - fuel_percentage),\n",
    "                fuel_bar_width,\n",
    "                fuel_bar_height * fuel_percentage,\n",
    "            ),\n",
    "        )  # Fuel bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Logging to ./ppo_lunar_landing_tensorboard/PPO_56\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 9273 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 0    |\n",
      "|    total_timesteps | 8192 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 3880        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008238109 |\n",
      "|    clip_fraction        | 0.0275      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.38       |\n",
      "|    explained_variance   | 0.000542    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.03e+03    |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00406    |\n",
      "|    value_loss           | 3.31e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 3260        |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008327667 |\n",
      "|    clip_fraction        | 0.0236      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.38       |\n",
      "|    explained_variance   | 0.00437     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 718         |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00275    |\n",
      "|    value_loss           | 3.41e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 3048        |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011147425 |\n",
      "|    clip_fraction        | 0.0532      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.37       |\n",
      "|    explained_variance   | 0.111       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 783         |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00349    |\n",
      "|    value_loss           | 2.93e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 2940        |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008662188 |\n",
      "|    clip_fraction        | 0.0719      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.35       |\n",
      "|    explained_variance   | 0.252       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 996         |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00321    |\n",
      "|    value_loss           | 2.33e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 2883        |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 17          |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015388316 |\n",
      "|    clip_fraction        | 0.222       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.31       |\n",
      "|    explained_variance   | 0.463       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 643         |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.00864    |\n",
      "|    value_loss           | 1.92e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 2834        |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 20          |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014407898 |\n",
      "|    clip_fraction        | 0.142       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.26       |\n",
      "|    explained_variance   | 0.544       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.02e+03    |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.00731    |\n",
      "|    value_loss           | 2.14e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 2806        |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 23          |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009977055 |\n",
      "|    clip_fraction        | 0.111       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.29       |\n",
      "|    explained_variance   | 0.573       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 763         |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0047     |\n",
      "|    value_loss           | 1.8e+03     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 2771        |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013092915 |\n",
      "|    clip_fraction        | 0.141       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.29       |\n",
      "|    explained_variance   | 0.694       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 850         |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00983    |\n",
      "|    value_loss           | 1.71e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 2742        |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009862854 |\n",
      "|    clip_fraction        | 0.076       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.24       |\n",
      "|    explained_variance   | 0.729       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 735         |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00747    |\n",
      "|    value_loss           | 1.83e+03    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 2726         |\n",
      "|    iterations           | 11           |\n",
      "|    time_elapsed         | 33           |\n",
      "|    total_timesteps      | 90112        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0078519685 |\n",
      "|    clip_fraction        | 0.0611       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.23        |\n",
      "|    explained_variance   | 0.751        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 766          |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | -0.00568     |\n",
      "|    value_loss           | 1.54e+03     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 2707        |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 36          |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010476732 |\n",
      "|    clip_fraction        | 0.0797      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.24       |\n",
      "|    explained_variance   | 0.75        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 623         |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0119     |\n",
      "|    value_loss           | 1.49e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 2678        |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 39          |\n",
      "|    total_timesteps      | 106496      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010057673 |\n",
      "|    clip_fraction        | 0.077       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.2        |\n",
      "|    explained_variance   | 0.802       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 572         |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.00741    |\n",
      "|    value_loss           | 1.43e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 2661        |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 43          |\n",
      "|    total_timesteps      | 114688      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007868498 |\n",
      "|    clip_fraction        | 0.0702      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.18       |\n",
      "|    explained_variance   | 0.874       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 517         |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0093     |\n",
      "|    value_loss           | 1.2e+03     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 2652        |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 46          |\n",
      "|    total_timesteps      | 122880      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006750866 |\n",
      "|    clip_fraction        | 0.0506      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.19       |\n",
      "|    explained_variance   | 0.913       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 388         |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.00611    |\n",
      "|    value_loss           | 1.24e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 2646        |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 49          |\n",
      "|    total_timesteps      | 131072      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012886623 |\n",
      "|    clip_fraction        | 0.15        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.25       |\n",
      "|    explained_variance   | 0.946       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 213         |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.014      |\n",
      "|    value_loss           | 911         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 2643        |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 52          |\n",
      "|    total_timesteps      | 139264      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010600928 |\n",
      "|    clip_fraction        | 0.104       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.16       |\n",
      "|    explained_variance   | 0.953       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 389         |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0105     |\n",
      "|    value_loss           | 875         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 2641        |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 55          |\n",
      "|    total_timesteps      | 147456      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009439839 |\n",
      "|    clip_fraction        | 0.0726      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.19       |\n",
      "|    explained_variance   | 0.97        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 234         |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.00878    |\n",
      "|    value_loss           | 856         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 2641        |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 58          |\n",
      "|    total_timesteps      | 155648      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011022415 |\n",
      "|    clip_fraction        | 0.0923      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.12       |\n",
      "|    explained_variance   | 0.982       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 171         |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.00812    |\n",
      "|    value_loss           | 685         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 2640         |\n",
      "|    iterations           | 20           |\n",
      "|    time_elapsed         | 62           |\n",
      "|    total_timesteps      | 163840       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075282482 |\n",
      "|    clip_fraction        | 0.0623       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.17        |\n",
      "|    explained_variance   | 0.985        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 191          |\n",
      "|    n_updates            | 190          |\n",
      "|    policy_gradient_loss | -0.00648     |\n",
      "|    value_loss           | 600          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 2641        |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 65          |\n",
      "|    total_timesteps      | 172032      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011087431 |\n",
      "|    clip_fraction        | 0.0943      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.08       |\n",
      "|    explained_variance   | 0.992       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 96.1        |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.00854    |\n",
      "|    value_loss           | 427         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 2643         |\n",
      "|    iterations           | 22           |\n",
      "|    time_elapsed         | 68           |\n",
      "|    total_timesteps      | 180224       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074159214 |\n",
      "|    clip_fraction        | 0.0845       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.19        |\n",
      "|    explained_variance   | 0.988        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 210          |\n",
      "|    n_updates            | 210          |\n",
      "|    policy_gradient_loss | -0.00616     |\n",
      "|    value_loss           | 444          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 2642        |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 71          |\n",
      "|    total_timesteps      | 188416      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009070516 |\n",
      "|    clip_fraction        | 0.127       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.12       |\n",
      "|    explained_variance   | 0.994       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 141         |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.00626    |\n",
      "|    value_loss           | 299         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 2638        |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 74          |\n",
      "|    total_timesteps      | 196608      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007187735 |\n",
      "|    clip_fraction        | 0.0551      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | 0.996       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 73.9        |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.00565    |\n",
      "|    value_loss           | 245         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 2628        |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 77          |\n",
      "|    total_timesteps      | 204800      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006876363 |\n",
      "|    clip_fraction        | 0.0612      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.08       |\n",
      "|    explained_variance   | 0.996       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 88.7        |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.00453    |\n",
      "|    value_loss           | 220         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 2623         |\n",
      "|    iterations           | 26           |\n",
      "|    time_elapsed         | 81           |\n",
      "|    total_timesteps      | 212992       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077161146 |\n",
      "|    clip_fraction        | 0.116        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.06        |\n",
      "|    explained_variance   | 0.996        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 60.3         |\n",
      "|    n_updates            | 250          |\n",
      "|    policy_gradient_loss | -0.0111      |\n",
      "|    value_loss           | 196          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 2617         |\n",
      "|    iterations           | 27           |\n",
      "|    time_elapsed         | 84           |\n",
      "|    total_timesteps      | 221184       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065644896 |\n",
      "|    clip_fraction        | 0.0717       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.04        |\n",
      "|    explained_variance   | 0.997        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 49.5         |\n",
      "|    n_updates            | 260          |\n",
      "|    policy_gradient_loss | -0.00545     |\n",
      "|    value_loss           | 178          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 2613        |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 87          |\n",
      "|    total_timesteps      | 229376      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010479365 |\n",
      "|    clip_fraction        | 0.179       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.05       |\n",
      "|    explained_variance   | 0.998       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 81          |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.0133     |\n",
      "|    value_loss           | 131         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 2612         |\n",
      "|    iterations           | 29           |\n",
      "|    time_elapsed         | 90           |\n",
      "|    total_timesteps      | 237568       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071938694 |\n",
      "|    clip_fraction        | 0.0814       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.05        |\n",
      "|    explained_variance   | 0.998        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 76.4         |\n",
      "|    n_updates            | 280          |\n",
      "|    policy_gradient_loss | -0.00686     |\n",
      "|    value_loss           | 168          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 2608        |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 94          |\n",
      "|    total_timesteps      | 245760      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012514247 |\n",
      "|    clip_fraction        | 0.157       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.962      |\n",
      "|    explained_variance   | 0.997       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 61.3        |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.00833    |\n",
      "|    value_loss           | 153         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 2601        |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 97          |\n",
      "|    total_timesteps      | 253952      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012638782 |\n",
      "|    clip_fraction        | 0.169       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.03       |\n",
      "|    explained_variance   | 0.997       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 48.6        |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0121     |\n",
      "|    value_loss           | 142         |\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "# Callback para registrar recompensas\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "import numpy as np\n",
    "\n",
    "class RewardLoggerCallback(BaseCallback):\n",
    "    def __init__(self, verbose=0):\n",
    "        super(RewardLoggerCallback, self).__init__(verbose)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if len(self.locals[\"infos\"]) > 0 and \"episode\" in self.locals[\"infos\"][0]:\n",
    "            episode_rewards = [info[\"episode\"][\"r\"] for info in self.locals[\"infos\"] if \"episode\" in info]\n",
    "            if episode_rewards:\n",
    "                mean_reward = np.mean(episode_rewards)\n",
    "                self.logger.record(\"reward/mean\", mean_reward)\n",
    "        return True\n",
    "\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
    "\n",
    "env = SubprocVecEnv([lambda: LunarLandingEnv() for _ in range(4)])\n",
    "# Criar o modelo\n",
    "model = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    env,\n",
    "    verbose=1,\n",
    "    tensorboard_log=\"./ppo_lunar_landing_tensorboard/\",\n",
    "    learning_rate=3e-4,\n",
    "    n_steps=2048,\n",
    "    batch_size=64,\n",
    "    gae_lambda=0.95,\n",
    "    gamma=0.99,\n",
    "    ent_coef=0.0,\n",
    "    clip_range=0.2,\n",
    ")\n",
    "# Treinar o agente\n",
    "model.learn(total_timesteps=250_000, callback=RewardLoggerCallback())\n",
    "# Salvar o modelo\n",
    "model.save(\"ppo_lunar_landing\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model = PPO.load(\"ppo_lunar_landing\")\n",
    "\n",
    "# Create a new environment for testing\n",
    "env = LunarLandingEnv()\n",
    "\n",
    "# Run the agent\n",
    "obs, _ = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    action, _ = model.predict(obs)\n",
    "    obs, reward, done, _, _ = env.step(action)\n",
    "    env.render()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def human_play():\n",
    "    import pygame\n",
    "    from pygame.locals import K_UP, K_LEFT, K_RIGHT, K_ESCAPE, QUIT\n",
    "\n",
    "    env = LunarLandingEnv()\n",
    "    env.reset()\n",
    "\n",
    "    # Inicializar Pygame\n",
    "    pygame.init()\n",
    "    screen = pygame.display.set_mode((env.screen_width, env.screen_height))\n",
    "    pygame.display.set_caption(\"Lunar Landing - Human Play\")\n",
    "    clock = pygame.time.Clock()\n",
    "\n",
    "    done = False\n",
    "    action = 0  # Ação inicial: Fazer nada\n",
    "\n",
    "    while not done:\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == QUIT:\n",
    "                done = True\n",
    "            elif event.type == pygame.KEYDOWN:\n",
    "                if event.key == K_ESCAPE:  # Sair do jogo\n",
    "                    done = True\n",
    "                elif event.key == K_UP:  # Empuxo para cima\n",
    "                    action = 1\n",
    "                elif event.key == K_LEFT:  # Empuxo para esquerda\n",
    "                    action = 2\n",
    "                elif event.key == K_RIGHT:  # Empuxo para direita\n",
    "                    action = 3\n",
    "            elif event.type == pygame.KEYUP:\n",
    "                if event.key in [K_UP, K_LEFT, K_RIGHT]:  # Parar de aplicar força\n",
    "                    action = 0\n",
    "\n",
    "        # Atualizar o ambiente com a ação selecionada\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        # Renderizar o ambiente\n",
    "        env.render()\n",
    "\n",
    "        # Verificar se o episódio terminou\n",
    "        if terminated or truncated:\n",
    "            print(\"Episódio terminou!\")\n",
    "            print(f\"Recompensa final: {reward}\")\n",
    "            break\n",
    "\n",
    "        # Limitar a taxa de quadros\n",
    "        clock.tick(env.metadata[\"render_fps\"])\n",
    "\n",
    "    # Fechar o ambiente\n",
    "    env.close()\n",
    "    pygame.quit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episódio terminou!\n",
      "Recompensa final: 5.0\n"
     ]
    }
   ],
   "source": [
    "human_play()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
